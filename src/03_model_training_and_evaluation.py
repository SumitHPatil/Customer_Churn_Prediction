# -*- coding: utf-8 -*-
"""model training and evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xdJOYK3Lr0GU7D2LEOULxnkwwqgp9E66

**Importing the prerequisites**
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import pickle
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.model_selection import RandomizedSearchCV

"""**Loading encoded dataset**"""

df = pd.read_csv('/content/Encoded_Customer_Churn.csv')
df

"""**Splitting data into training and testing sets**"""

#splitting target and features
x = df.drop('Churn',axis=1)
y = df['Churn']

#train test split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

#understanding imbalance
print(y_train.value_counts())
print(y_test.value_counts())

"""**SMOTE** techinque

as we know there is imbalance in target column
"""

smote = SMOTE(random_state=42)
x_train_smote,y_train_smote = smote.fit_resample(x_train,y_train)

# after smote
print(y_train_smote.value_counts())

"""**Model training**"""

models = {

    "DecisionTree" : DecisionTreeClassifier(random_state=42),
    "RandomForest" : RandomForestClassifier(random_state=42),
    "XGBoost" : XGBClassifier(random_state=42)
}

#dictionary to store cross validation score
cv_scores = {}

#perform 5 fold cross validation for all models
for model_name,model in models.items():
  print(f"Training {model_name} with default parameters")
  scores= cross_val_score(model,x_train_smote,y_train_smote,cv=5,scoring="accuracy")
  cv_scores[model_name] = scores
  print(f"Cross validation scores for {model_name} : {np.mean(scores):.2f}")
  print("-"*70)

print('--- Model Evaluation ---')
predictions = {}

for model_name, model in models.items():
    print(f"Training {model_name} model...")
    model.fit(x_train_smote, y_train_smote)
    model_predictions = model.predict(x_test)
    predictions[model_name] = model_predictions
    print(f"{model_name} training complete and predictions made.\n")

# Evaluate model performance
for model_name, model_predictions in predictions.items():
    print(f"--- {model_name} Performance ---")
    accuracy = accuracy_score(y_test, model_predictions)
    conf_matrix = confusion_matrix(y_test, model_predictions)
    class_report = classification_report(y_test, model_predictions)

    print(f"Accuracy: {accuracy:.2f}")
    print("Confusion Matrix:")
    print(conf_matrix)
    print("Classification Report:")
    print(class_report)
    print("\n")

param_dist_dt = {
    'max_depth': list(range(3, 21)),
    'min_samples_split': list(range(2, 11)),
    'min_samples_leaf': list(range(1, 6))
}

print("Hyperparameter distribution for Decision Tree:")
print(param_dist_dt)

"""## Define Hyperparameter Distributions for Random Forest

Define the hyperparameter search space for the Random Forest Classifier, including parameters like `n_estimators`, `max_depth`, `min_samples_split`, and `min_samples_leaf`.

"""

param_dist_rf = {
    'n_estimators': list(range(100, 501, 50)),
    'max_depth': list(range(5, 21)),
    'min_samples_split': list(range(2, 11)),
    'min_samples_leaf': list(range(1, 6))
}

print("Hyperparameter distribution for Random Forest:")
print(param_dist_rf)

"""## Define Hyperparameter Distributions for XGBoost

Define the hyperparameter search space for the XGBoost Classifier, including parameters like `n_estimators`, `max_depth`, `learning_rate`, and `subsample`.

"""

param_dist_xgb = {
    'n_estimators': list(range(100, 501, 50)),
    'max_depth': list(range(3, 11)),
    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0]
}

print("Hyperparameter distribution for XGBoost:")
print(param_dist_xgb)

"""## Perform Random Search for Decision Tree

Execute Random Search Cross-Validation for the Decision Tree Classifier using the defined hyperparameter distributions, `x_train_smote`, `y_train_smote`, and an appropriate scoring metric (e.g., 'accuracy').

"""

# DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(random_state=42)

#  RandomizedSearchCV for Decision Tree
random_search_dt = RandomizedSearchCV(
    estimator=dt_classifier,
    param_distributions=param_dist_dt,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1  # Use all available processors
)

# Fit RandomizedSearchCV to the SMOTE-resampled training data
print("Performing Random Search for Decision Tree...")
random_search_dt.fit(x_train_smote, y_train_smote)
print("Random Search for Decision Tree complete.")

# Print the best parameters and best score
print("Best parameters for Decision Tree:", random_search_dt.best_params_)
print("Best cross-validation accuracy for Decision Tree:", random_search_dt.best_score_)
print("-"*70)

"""## Perform Random Search for Random Forest

Execute Random Search Cross-Validation for the Random Forest Classifier using the defined hyperparameter distributions, `x_train_smote`, `y_train_smote`, and an appropriate scoring metric (e.g., 'accuracy').

"""

# RandomForestClassifier
rf_classifier = RandomForestClassifier(random_state=42)

# RandomizedSearchCV for Random Forest
random_search_rf = RandomizedSearchCV(
    estimator=rf_classifier,
    param_distributions=param_dist_rf,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1  # Use all available processors
)

# Fit RandomizedSearchCV to the SMOTE-resampled training data
print("Performing Random Search for Random Forest...")
random_search_rf.fit(x_train_smote, y_train_smote)
print("Random Search for Random Forest complete.")

# Print the best parameters and best score
print("Best parameters for Random Forest:", random_search_rf.best_params_)
print("Best cross-validation accuracy for Random Forest:", random_search_rf.best_score_)

"""## Perform Random Search for XGBoost

Execute Random Search Cross-Validation for the XGBoost Classifier using the defined hyperparameter distributions, `x_train_smote`, `y_train_smote`, and an appropriate scoring metric (e.g., 'accuracy').


"""

#  XGBClassifier
xgb_classifier = XGBClassifier(random_state=42)

#  RandomizedSearchCV for XGBoost
random_search_xgb = RandomizedSearchCV(
    estimator=xgb_classifier,
    param_distributions=param_dist_xgb,
    n_iter=100,
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1  # Use all available processors
)

# Fit RandomizedSearchCV to the SMOTE-resampled training data
print("Performing Random Search for XGBoost...")
random_search_xgb.fit(x_train_smote, y_train_smote)
print("Random Search for XGBoost complete.")

# Print the best parameters and best score
print("Best parameters for XGBoost:", random_search_xgb.best_params_)
print("Best cross-validation accuracy for XGBoost:", random_search_xgb.best_score_)

"""## Evaluate Best Models from Random Search

Retrieve the best estimator from each Random Search, then evaluate its performance on the `x_test` and `y_test` datasets. This evaluation will include accuracy, confusion matrix, and a classification report for each optimized model.

"""

# Create a dictionary to store the best estimators
best_models = {
    'DecisionTree': random_search_dt.best_estimator_,
    'RandomForest': random_search_rf.best_estimator_,
    'XGBoost': random_search_xgb.best_estimator_
}

print("\n--- Evaluation of Best Models from Random Search ---")

# Iterate through each best model and evaluate its performance
for model_name, best_model in best_models.items():
    print(f"\n--- {model_name} (Optimized) Performance ---")

    # Make predictions on the test set
    y_pred = best_model.predict(x_test)
    y_pred_proba = best_model.predict_proba(x_test)[:, 1] # Probability of the positive class (churn=1)

    # Calculate and print evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

    print(f"Accuracy: {accuracy:.2f}")
    print("Confusion Matrix:")
    print(conf_matrix)
    print("Classification Report:")
    print(class_report)
    print("Sample Predicted Probabilities (for Churn=1):")
    print(y_pred_proba[:5]) # Display first 5 probabilities
    print("\n")

"""**Choosing model**

Save the best performing model (XGBoost) using `pickle` for future use, and then demonstrate how to load this saved model and use it to make predictions on a new sample input.
"""

import pickle

# Access the best performing XGBoost model
best_xgb_model = best_models['XGBoost']

# Define the file path for saving the model
model_filepath = '/content/xgboost_model.pkl'

# Save the model using pickle
with open(model_filepath, 'wb') as file:
    pickle.dump(best_xgb_model, file)

print(f"Best XGBoost model saved to {model_filepath}")

"""## Load the Saved Model and Make Predictions

Load the previously saved XGBoost model from the `xgboost_model.pkl` file and use it to make predictions on a new, hypothetical sample input. This demonstrates the model's functionality after persistence.
"""

import pickle

# Define the file path for the saved model
model_filepath = '/content/xgboost_model.pkl'

# Load the model from the file
with open(model_filepath, 'rb') as file:
    loaded_xgb_model = pickle.load(file)

print(f"XGBoost model loaded from {model_filepath}")


# For demonstration, we'll use the first row of the test set as a new sample

new_sample_input = x_test.iloc[0:1]

print("\nNew sample input for prediction:")
print(new_sample_input)

# Make a prediction using the loaded model
prediction = loaded_xgb_model.predict(new_sample_input)
prediction_proba = loaded_xgb_model.predict_proba(new_sample_input)

predicted_class = prediction[0]
probability_of_predicted_class = prediction_proba[0][predicted_class]

print(f"\nPrediction for the new sample input: {predicted_class}")
print(f"Probability of the predicted class ({predicted_class}): {probability_of_predicted_class:.4f}")

print("-"*70)

"""**performs well and hence project comes to an end with developed prediction system**

### **Final summary**

Among the models evaluated after hyperparameter tuning,

*   XGBoost performed the best on the test data with an accuracy of **0.79**.
*    It was followed by Random Forest with an accuracy of **0.77**.

*   the Decision Tree model with an accuracy of **0.74**.
"""